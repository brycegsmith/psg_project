{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psg import PSG\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import constants\n",
    "from ecg import ECG\n",
    "from emg import EMG\n",
    "import eeg\n",
    "from eeg import EEG\n",
    "from eog import EOG\n",
    "from sao2 import *\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from c:\\Users\\andre\\OneDrive\\Desktop\\CS4641\\Project\\PSG_Project\\psg_project\\plm6.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\signal\\_spectral_py.py:1961: UserWarning: nperseg = 1048576 is greater than input length  = 15360, using nperseg = 15360\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n",
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\heartpy\\analysis.py:522: UserWarning: Short signal.\n",
      "\n",
      "---------Warning:---------\n",
      "too few peak-peak intervals for (reliable) frequency domain measure computation, frequency output measures are still computed but treat them with caution!\n",
      "\n",
      "HF is usually computed over a minimum of 1 minute of good signal. LF is usually computed over a minimum of 2 minutes of good signal.VLF is usually computed over a minimum of 5 minutes of good signal.The LF/HF ratio is usually computed over minimum 24 hours, although an absolute minimum of 5 min has also been suggested.\n",
      "\n",
      "For more info see: \n",
      "Shaffer, F., Ginsberg, J.P. (2017), An Overview of Heart Rate Variability Metrics and Norms.\n",
      "\n",
      "Task Force of Pacing and Electrophysiology (1996), Heart Rate Variability, in: European Heart Journal, vol.17, issue 3, pp354-381\n",
      "\n",
      "This warning will not repeat\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\interpolate\\_fitpack2.py:280: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\interpolate\\_fitpack2.py:280: UserWarning: \n",
      "A theoretically impossible result was found during the iteration\n",
      "process for finding a smoothing spline with fp = s: s too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plm6 successfully exported!\n"
     ]
    }
   ],
   "source": [
    "training_set_1 = ['ins2', 'ins5', 'ins6', 'ins7', 'n11', 'n2']\n",
    "training_set_2 = ['nfle10', 'nfle11', 'nfle12', 'nfle13', 'nfle14', 'nfle15']\n",
    "training_set_3 = ['plm10', 'plm1', 'plm2', 'plm3', 'plm5', 'plm6']\n",
    "training_set_4 = ['rbd10', 'rbd11', 'rbd12', 'rbd13', 'rbd16', 'rbd17']\n",
    "\n",
    "testing_set_1 = ['ins8', 'ins9', 'n5'] \n",
    "testing_set_2 = ['nfle16', 'nfle17', 'nfle18', 'nfle19'] \n",
    "testing_set_3 = ['plm7', 'plm8', 'plm9']\n",
    "testing_set_4 = ['rbd18', 'rbd19', 'rbd20']\n",
    "\n",
    "# Make sure you have the edf and txt files for each individual along with the GitHub modules.\n",
    "\n",
    "for i in training_set_2:\n",
    "    psg_data = PSG(i)\n",
    "\n",
    "    #EEG Feature Extraction\n",
    "    reload(eeg)\n",
    "    EEG_columns = eeg.EEG(psg_data.data)\n",
    "    EEG_columns.extract_features()\n",
    "    eeg_data = EEG_columns.eeg_features\n",
    "\n",
    "    #SAO2 Feature Extraction\n",
    "    oxyData = psg_data.data[['epoch','SAO2']]\n",
    "    sao2_data = SAO2(oxyData).get_SAO2_metrics()\n",
    "\n",
    "    #EOG Feature Extraction\n",
    "    eog_data = EOG(psg_data.data).get_EOG_metrics()\n",
    "\n",
    "    #EMG Feature Extraction\n",
    "    sxDfInput = psg_data.data.loc[:, [\"elapsed_seconds\", \"epoch\", \"SX1-SX2\"]]\n",
    "    sx = EMG(sxDfInput, signalType = \"SX1-SX2\")\n",
    "    sx_data = sx.getMetrics()\n",
    "\n",
    "    dxDfInput = psg_data.data.loc[:, [\"elapsed_seconds\", \"epoch\", \"DX1-DX2\"]]\n",
    "    dx = EMG(dxDfInput, signalType = \"DX1-DX2\")\n",
    "    dx_data = dx.getMetrics()\n",
    "\n",
    "    emgDfInput = psg_data.data.loc[:, [\"elapsed_seconds\", \"epoch\", \"EMG1-EMG2\"]]\n",
    "    emg_columns = EMG(emgDfInput)\n",
    "    emg_data = emg_columns.getMetrics()\n",
    "\n",
    "    #ECG Feature Extraction\n",
    "    ecgDfInput = psg_data.data.loc[:, [\"epoch\", \"ECG1-ECG2\"]]\n",
    "    ecg_columns = ECG(ecgDfInput)\n",
    "    ecg_data = ecg_columns.getMetrics()\n",
    "\n",
    "    plethDfInput = psg_data.data.loc[:, [\"elapsed_seconds\", \"epoch\", \"PLETH\"]]\n",
    "    pleth = ECG(plethDfInput, signalType = 'PLETH')\n",
    "    pleth_data = pleth.getMetrics()\n",
    "\n",
    "    #Combine and Export Data\n",
    "    stage_1 = pd.merge(psg_data.txtData, eeg_data, how = 'left', on='epoch')\n",
    "    stage_2 = pd.merge(stage_1, sao2_data, how = 'left', on='epoch')\n",
    "    stage_3 = pd.merge(stage_2, eog_data, how = 'left', on='epoch')\n",
    "    stage_4 = pd.merge(stage_3, sx_data, how = 'left', on='epoch')\n",
    "    stage_5 = pd.merge(stage_4, dx_data, how = 'left', on='epoch')\n",
    "    stage_6 = pd.merge(stage_5, emg_data, how = 'left', on='epoch')\n",
    "    stage_7 = pd.merge(stage_6, ecg_data, how = 'left', on='epoch')\n",
    "    final = pd.merge(stage_7, pleth_data, how = 'left', on='epoch')\n",
    "\n",
<<<<<<< HEAD
    "    final.to_csv(i+'.csv')\n",
    "    print(i+\" successfully exported!\")\n"
=======
    "        #Combine and Export Data\n",
    "        stage_1 = pd.merge(psg_data.txtData, eeg_data, how = 'left', on='epoch')\n",
    "        stage_2 = pd.merge(stage_1, sao2_data, how = 'left', on='epoch')\n",
    "        stage_3 = pd.merge(stage_2, eog_data, how = 'left', on='epoch')\n",
    "        stage_4 = pd.merge(stage_3, sx_data, how = 'left', on='epoch')\n",
    "        stage_5 = pd.merge(stage_4, dx_data, how = 'left', on='epoch')\n",
    "        stage_6 = pd.merge(stage_5, emg_data, how = 'left', on='epoch')\n",
    "        stage_7 = pd.merge(stage_6, ecg_data, how = 'left', on='epoch')\n",
    "        final = pd.merge(stage_7, pleth_data, how = 'left', on='epoch')\n",
    "\n",
    "        final.to_csv(i+'.csv')\n",
    "        print(i+\" successfully exported!\")\n",
    "\n",
    "        try:\n",
    "            del psg_data\n",
    "            gc.collect()\n",
    "        except:\n",
    "            print(\"Garbage collection failed\")\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print(i+\" failed to export.\")\n",
    "        continue\n"
>>>>>>> 5482bd47ee903033f2b5a5b5264a2f373403d139
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation - Box Cox followed by Min-Max Scaling\n",
    "final = pd.read_csv(i+'.csv')\n",
    "\n",
    "final.dropna(subset = ['hf_PLETH'], inplace=True)\n",
    "\n",
    "for column in final.columns[5:]:\n",
    "    final[column] = final[column].abs()\n",
    "        \n",
    "    try:\n",
    "        box_cox_trans = stats.boxcox(final[column])[0]\n",
    "        normalized = ((box_cox_trans-box_cox_trans.mean())/box_cox_trans.std())\n",
    "        final[column] = (normalized-normalized.min())/(normalized.max()-normalized.min())\n",
    "        \n",
    "    except:\n",
    "        normalized = (final[column]-final[column].mean()) / final[column].std()\n",
    "        final[column] = (normalized-normalized.min())/(normalized.max()-normalized.min())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Encoding of Gender/Condition and Ordinal Encoding of Sleep Stage\n",
    "# Drop 1st column (previous index from processing)\n",
    "final.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "# Add columnS for sleep stage\n",
    "final.insert(2, \"sleep_stage\", np.zeros(len(final)))\n",
    "\n",
    "# Replace sleep stage string with numerical value\n",
    "for stage in constants.SLEEP_STAGES:\n",
    "    final.loc[final[\"Sleep Stage\"] == stage, \"sleep_stage\"] = constants.SLEEP_STAGES.get(stage)\n",
    "\n",
    "# Drop original sleep stage column\n",
    "final.drop(\"Sleep Stage\", axis=1, inplace=True)\n",
    "\n",
    "# Replace sleep stage string with numerical value\n",
    "for condition in constants.CONDITION_TO_BINARY:\n",
    "    # Only check 1st row to confirm condition\n",
    "    if condition == final.at[0, \"condition\"]:\n",
    "        binary = constants.CONDITION_TO_BINARY.get(condition)\n",
    "\n",
    "        # 1st Binary digit\n",
    "        if binary[0] == 0:\n",
    "            final.insert(3, \"condition_0\", np.zeros(len(final)))\n",
    "        else:\n",
    "            final.insert(3, \"condition_0\", np.ones(len(final)))\n",
    "\n",
    "        # 2nd Binary digit\n",
    "        if binary[1] == 0:\n",
    "            final.insert(4, \"condition_1\", np.zeros(len(final)))\n",
    "        else:\n",
    "            final.insert(4, \"condition_1\", np.ones(len(final)))\n",
    "\n",
    "        # 3rd Binary digit\n",
    "        if binary[2] == 0:\n",
    "            final.insert(5, \"condition_2\", np.zeros(len(final)))\n",
    "        else:\n",
    "            final.insert(5, \"condition_2\", np.ones(len(final)))\n",
    "            \n",
    "\n",
    "# Drop original sleep stage column\n",
    "final.drop(\"condition\", axis=1, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.10.0"
=======
   "version": "3.9.1"
>>>>>>> 5482bd47ee903033f2b5a5b5264a2f373403d139
  },
  "vscode": {
   "interpreter": {
    "hash": "08e595c52ca3b9470036b1110e67b559e55f367cabc363f2e28d35631ed95060"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
